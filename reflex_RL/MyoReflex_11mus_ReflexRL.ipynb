{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433c7ac7",
   "metadata": {},
   "source": [
    "# Reflex RL - Modulating reflex module gains with RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb11c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2df290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ReflexInterface_RL\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "#from stable_baselines3.common.utils import get_device\n",
    "\n",
    "import os\n",
    "import skvideo.io\n",
    "import copy\n",
    "\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "def show_video(video_path, video_width = 500):\n",
    "    video_file = open(video_path, \"r+b\").read()\n",
    "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
    "    return HTML(f\"\"\"<video autoplay width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n",
    "\n",
    "register(\n",
    "    id=\"MyoReflex_RL-v0\",\n",
    "    entry_point=\"ReflexInterface_RL:ReflexEnv\",\n",
    "    max_episode_steps=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdb835",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# param_filename = 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best'\n",
    "# params_0 = np.loadtxt(f\"reflex_RL/reflex_param/{param_filename}.txt\")\n",
    "params_0 = np.ones(73,)\n",
    "\n",
    "test = gymnasium.make('MyoReflex_RL-v0', init_pose='walk', dt=0.01, mode='2D', tgt_field_ver=0, reflex_params=params_0, delta_control_mode='sym')\n",
    "test.reset()\n",
    "\n",
    "check_env(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3533512-b623-480b-9f30-de5bdc1eae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_0 = np.loadtxt(os.path.join(os.getcwd(),'..','reflex_RL', 'reflex_param', \n",
    "                                   'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best.txt')) # Load your params\n",
    "\n",
    "train_env = gymnasium.make('MyoReflex_RL-v0', render_mode=None, reflex_params=params_0, init_pose='walk', dt=0.01, \n",
    "                           mode='2D', tgt_field_ver=0, \n",
    "                           episode_limit=20, \n",
    "                           reward_wt=None,\n",
    "                           obs_param=None, rew_type=0, stim_mode='reflex', \n",
    "                           tgt_vel_mode='eval', sine_vel_args=None, delta_mode='delayed', delta_control_mode='sym')\n",
    "train_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1be463-d500-4c71-8d71-9a2bbaa4ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", train_env, learning_rate=0.0001, n_steps=4096, \n",
    "    batch_size=1024, n_epochs=4, gae_lambda=0.99, target_kl=0.01,\n",
    "    policy_kwargs = dict(net_arch=[128,128]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c6ccb3-9d40-4441-ac4c-fae08c494d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.learn(total_timesteps=10000)\n",
    "# #train_env.unwrapped.JNT_OPTIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c08d37c-2293-4b4f-8bb7-bd3cda6ceaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run clean\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "param_filename = 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best'\n",
    "params = np.loadtxt(f\"reflex_param/{param_filename}.txt\")\n",
    "\n",
    "eval_env = gymnasium.make('MyoReflex_RL-v0', init_pose='walk', dt=0.01, mode='2D', tgt_field_ver=0, reflex_params=params, episode_limit=2000, \n",
    "                          stim_mode='reflex', tgt_vel_mode='eval', delta_mode='delayed', delta_control_mode='sym')\n",
    "obs, _ = eval_env.reset()\n",
    "test_model = PPO.load(\"PPO_outputV3\\PPO_2024Nov29_1816_2D_TrainedModel\", env=eval_env)\n",
    "\n",
    "\n",
    "frames = []\n",
    "# obs = eval_env.reset()\n",
    "for timestep in np.arange(2000):\n",
    "    frame = eval_env.MyoEnv.sim.renderer.render_offscreen(camera_id=1)\n",
    "    frames.append(frame)\n",
    "\n",
    "    action, _states = test_model.predict(obs)\n",
    "    obs, rewards, is_done, info, _ = eval_env.step(action)\n",
    "\n",
    "    if is_done:\n",
    "        #print(f\"Succed at {time_step}\")\n",
    "        break\n",
    "\n",
    "\n",
    "param_filename = '2D_PPO_Test'\n",
    "skvideo.io.vwrite(f\"{param_filename}.mp4\",\n",
    "                  np.asarray(frames),inputdict={\"-r\":\"100\"}, outputdict={\"-r\" : \"100\", \"-pix_fmt\": \"yuv420p\"})\n",
    "show_video(f\"{param_filename}.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f18c96-f656-4f24-9fe6-bb1c96bc7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save states\n",
    "\n",
    "# Wang algorithm\n",
    "#reward_type = 1\n",
    "#rw_and_wts = dict(zip(['v_tgt'], [1]) )\n",
    "\n",
    "params_0 = np.loadtxt(os.path.join(os.getcwd(),'..','reflex_RL', 'reflex_param', 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best.txt')) # Load your params\n",
    "# \n",
    "# Others\n",
    "reward_type = 0\n",
    "rw_and_wts = None\n",
    "\n",
    "rollout_env = gymnasium.make('MyoReflex_RL-v0', init_pose='walk', dt=0.01, mode='2D', tgt_field_ver=0, reflex_params=params_0, episode_limit=2000, \n",
    "                          obs_param=None, rew_type=reward_type, reward_wt=rw_and_wts, stim_mode='reflex', \n",
    "                          tgt_vel_mode='eval', sine_vel_args=None, delta_mode='delayed', delta_control_mode='asym')\n",
    "# check_env(rollout_env)\n",
    "#rollout_env.reset()\n",
    "#rollout_env.run_reflex_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cea4569",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_env.reset()\n",
    "\n",
    "frames = []\n",
    "pelvisV = []\n",
    "\n",
    "timesteps = int(20 / rollout_env.dt)\n",
    "\n",
    "for i in range(timesteps):\n",
    "    rollout_env.MyoEnv.sim.data.camera(4).xpos[2] = 2.181\n",
    "    frame = rollout_env.MyoEnv.sim.renderer.render_offscreen(camera_id=4)\n",
    "    frames.append(frame)\n",
    "\n",
    "    pelvisV_curr = rollout_env.SENSOR_DATA['body']['pelvis_vel']\n",
    "    pelvisV.append(pelvisV_curr)\n",
    "\n",
    "    _ , is_done = rollout_env.run_reflex_step()\n",
    "\n",
    "    if is_done:\n",
    "        print(f\"Stopped at {i}\")\n",
    "        break\n",
    "\n",
    "skvideo.io.vwrite(\"Test.mp4\", \n",
    "                  np.asarray(frames),inputdict={\"-r\":\"100\"}, outputdict={\"-r\" : \"100\", \"-pix_fmt\": \"yuv420p\"})\n",
    "# show in the notebook\n",
    "show_video(\"Test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c4ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from above dt = 0.01\n",
    "dt = 0.01\n",
    "\n",
    "first_values = [array[0] for array in pelvisV]\n",
    "indices = np.arange(len(first_values))*dt\n",
    "\n",
    "window_size = 400\n",
    "moving_avg = np.convolve(first_values, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.plot(indices, first_values, label='First Value', alpha=0.6)\n",
    "plt.plot(indices[window_size-1:], moving_avg, label=f'Moving Average ({window_size} indices)', color='orange', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Pelvis Velocity (m/s)')\n",
    "plt.title('Pelvis Velocity')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81659c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout_env.reset()\n",
    "\n",
    "frames = []\n",
    "\n",
    "timesteps = int(20 / rollout_env.dt)\n",
    "\n",
    "for i in range(timesteps):\n",
    "    rollout_env.MyoEnv.sim.data.camera(4).xpos[2] = 2.181\n",
    "    frame = rollout_env.MyoEnv.sim.renderer.render_offscreen(camera_id=4)\n",
    "    frames.append(frame)\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: Data capture here\n",
    "    exo_r_torque = MyoEnv.env.sim.data.actuator('Exo_R').actuator_force[0]\n",
    "    MyoEnv.env.sim.data.joint('r_ankle').qpos[0]\n",
    "    \"\"\"\n",
    "\n",
    "    _ , is_done = rollout_env.run_reflex_step()\n",
    "\n",
    "    if is_done:\n",
    "        print(f\"Stopped at {i}\")\n",
    "        break\n",
    "\n",
    "skvideo.io.vwrite(\"Test.mp4\", \n",
    "                  np.asarray(frames),inputdict={\"-r\":\"100\"}, outputdict={\"-r\" : \"100\", \"-pix_fmt\": \"yuv420p\"})\n",
    "# show in the notebook\n",
    "show_video(\"Test.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1493583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 'No_Exo_Retrain', 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best.txt'\n",
    "\n",
    "# Test Initialize with parts of gait cycle\n",
    "# Collect rollouts of best walking from reflex ctrl\n",
    "# rollout_env.reset()\n",
    "# state_list, reflex_list, ref_act_list, grf_list = rollout_env.collect_reflex_rollouts()\n",
    "\n",
    "# # Save environment and reflex controller state\n",
    "\n",
    "# print(len(grf_list))\n",
    "\n",
    "# 0 - Right\n",
    "# 1 - Left\n",
    "# plt.plot(range(2000), grf_list[:,0], label='Right')\n",
    "# plt.plot(range(2000), grf_list[:,1], label='Left')\n",
    "# plt.xlim([1233, 1948])\n",
    "# plt.legend()\n",
    "\n",
    "# # Check the GRF for segmenting\n",
    "# np.where(grf_list[1233:1948,0] > 0)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(grf_list[1233:1948, 0])\n",
    "# plt.plot(grf_list[1233:1948, 1])\n",
    "\n",
    "# # Extract based on GRF values\n",
    "# save_state_list = state_list[1233:1948]\n",
    "# save_reflex_list = reflex_list[1233:1948]\n",
    "\n",
    "# np.save('save_state_list_2D.npy', save_state_list)\n",
    "# np.save('save_reflex_list_2D.npy', save_reflex_list)\n",
    "\n",
    "# # ------ Rendering check -----\n",
    "# # Rendering to check\n",
    "# # Testing of state reset\n",
    "# # 1. Set state\n",
    "# # 2. Run step normally\n",
    "# frames = []\n",
    "\n",
    "# env.reset()\n",
    "\n",
    "\"\"\"\n",
    "# !! REMEMBER TO Disable warmstart for Mujoco in the XML file options\n",
    "# https://github.com/google-deepmind/mujoco/issues/493\n",
    "\"\"\"\n",
    "\n",
    "rollout_env.reset()\n",
    "\n",
    "# loadedState = np.load(\"save_state_list_2D.npy\", allow_pickle=True)\n",
    "# loadedReflex = np.load(\"save_reflex_list_2D.npy\", allow_pickle=True)\n",
    "test_state = copy.deepcopy(loadedState[40])\n",
    "test_reflex = copy.deepcopy(loadedReflex[40])\n",
    "\n",
    "rollout_env.set_reflex_env_state(test_state)\n",
    "rollout_env.unwrapped.ReflexCtrl = test_reflex\n",
    "\n",
    "test_frame = rollout_env.MyoEnv.unwrapped.sim.renderer.render_offscreen(camera_id=4)\n",
    "\n",
    "plt.imshow(test_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6254f9-4bdb-4722-95bd-a4f68d62c04f",
   "metadata": {},
   "source": [
    "# Scripts to evaluate and generate videos of policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027db79d-9468-42cc-9b66-1f5cc69838df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect actions from best and worst policy\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import copy\n",
    "import skvideo.io\n",
    "\n",
    "def evaluate_trained_policy(test_env, test_policy, max_timestep, n_eval_ep, vel_sequence):\n",
    "    print(f\"Max timestep : {max_timestep}, \")\n",
    "    best_action = np.zeros((1, 94))\n",
    "    longest_action = np.zeros((1, 94))\n",
    "    action_queue = np.zeros((1, 94))\n",
    "    all_ep_len = []\n",
    "    all_rewards = []\n",
    "    ep_reward = 0\n",
    "    best_reward = 0\n",
    "    longest_reward = 0\n",
    "\n",
    "    frames = []\n",
    "    #act_list = np.zeros((max_timestep,22))\n",
    "    obs_list = np.zeros((max_timestep,25))\n",
    "    \n",
    "    obs_duringEval = 0\n",
    "    obs_replayEval = 0\n",
    "    obs_replaySaved = 0\n",
    "\n",
    "    act_mat = np.zeros((max_timestep,94))\n",
    "    \n",
    "    #print(f\"In func:{test_env.target_x_vel}\")\n",
    "    test_env.unwrapped.eval_x_vel = vel_sequence[0]\n",
    "    \n",
    "    for eps in range(n_eval_ep):\n",
    "        obs,_ = test_env.reset()\n",
    "        #print(f\"Policy_obs : {obs}\")\n",
    "        \n",
    "        #print(f\"In loop:{test_env.target_x_vel}\")\n",
    "        \n",
    "        if eps % np.int32(n_eval_ep/4) == 0:\n",
    "            print(f\"Eval Episode: {eps}\")\n",
    "        #print(f\"Eval Episode: {eps}\")\n",
    "        \n",
    "        for timestep in range(max_timestep):\n",
    "            test_env.unwrapped.eval_x_vel = vel_sequence[timestep] # Override target velocity with input sequence for evaluation\n",
    "            \n",
    "            obs_list[timestep, :] = obs.copy()\n",
    "            #print(f\"tgt_vel {obs[0]}\")\n",
    "            action, _states = test_policy.predict(obs, deterministic=True)\n",
    "            obs, rewards, is_done, failed, info = test_env.step(action)\n",
    "            \n",
    "            # Collect data\n",
    "            if action_queue.shape[0] != 1:\n",
    "                action_queue = np.vstack((action_queue, action))\n",
    "            else:\n",
    "                action_queue = action\n",
    "    \n",
    "            ep_reward += rewards\n",
    "            \n",
    "            #if timestep % 100 == 0:\n",
    "            #    print(f\"Time is {timestep}\")\n",
    "            \n",
    "            #if failed:\n",
    "            #    print(f\"Stopped at {timestep}\")\n",
    "            #    break\n",
    "                \n",
    "            if is_done:\n",
    "                print(f\"Succed at {timestep}\")\n",
    "                break\n",
    "        \n",
    "        if action_queue.shape[0] > best_action.shape[0] and best_reward < ep_reward:\n",
    "            best_action = action_queue.copy()\n",
    "            best_reward = ep_reward\n",
    "\n",
    "        # Collect best longest surviving\n",
    "        if action_queue.shape[0] > longest_action.shape[0]:\n",
    "            longest_action = action_queue.copy()\n",
    "            longest_reward = ep_reward\n",
    "    \n",
    "        all_ep_len.append(action_queue.shape[0])\n",
    "        all_rewards.append(ep_reward)\n",
    "\n",
    "        ep_mean = np.mean(np.array(all_ep_len))\n",
    "        ep_std = np.std(np.array(all_ep_len))\n",
    "\n",
    "        rew_mean = np.mean(np.array(all_rewards))\n",
    "        rew_std = np.std(np.array(all_rewards))\n",
    "        \n",
    "        ep_reward = 0\n",
    "        action_queue = np.zeros((1, 94))\n",
    "\n",
    "        print(f\"Best length : {best_action.shape[0]}\")\n",
    "    \n",
    "    eval_dict = {'obs': obs_list, 'best_action_seq': best_action, 'best_reward': best_reward, \n",
    "                 'longest_action_seq': longest_action, 'longest_reward': longest_reward, \n",
    "                 'all_ep_len': all_ep_len, 'all_rewards': all_rewards, \n",
    "                 'ep_mean': ep_mean, 'ep_std': ep_std, 'rew_mean': rew_mean, 'rew_std': rew_std}\n",
    "    \n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a46ab-05c8-4ef1-9186-ed12ad8c02e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def evaluate_30Hz_policy(test_env, test_policy, max_timestep, vel_sequence, path, filename, reward_wt_param):\n",
    "    frames = []\n",
    "    print(f\"Max timestep : {max_timestep}, \")\n",
    "    action_queue = np.zeros((1, 36))\n",
    "    ep_reward = 0\n",
    "    \n",
    "    test_env.unwrapped.eval_x_vel = vel_sequence[0]\n",
    "    eval_vec = np.zeros((max_timestep, 8))\n",
    "    \n",
    "    obs, _ = test_env.reset()\n",
    "    obs_list = np.zeros((max_timestep,25))\n",
    "\n",
    "    print(f\"Ep limit: {test_env.unwrapped.episode_limit}\")\n",
    "    \n",
    "    for timestep in range(max_timestep):\n",
    "        # Setting target velocities manually in the observation for the policy \n",
    "        # \n",
    "        #obs[0] = vel_sequence[timestep]\n",
    "        test_env.unwrapped.eval_x_vel = vel_sequence[timestep]\n",
    "        \n",
    "        #frame = test_env.MyoEnv.sim.renderer.render_offscreen(camera_id=1)\n",
    "        #frames.append(frame)\n",
    "        \n",
    "        # if timestep % 3 == 0:\n",
    "        #     action, _states = test_policy.predict(obs, deterministic=True)\n",
    "        #     stored_action = action\n",
    "        #     #print(f\"Action at {timestep}\")\n",
    "        action, _states = test_policy.predict(obs, deterministic=True)\n",
    "        \n",
    "        obs_list[timestep, :] = obs.copy()\n",
    "\n",
    "        obs, rewards, is_done, failed, info = test_env.step(action) # have the same delta action for 3 timesteps\n",
    "        #obs, rewards, is_done, failed, info = test_env.step(np.zeros(94,)) # Debugging forward velocity\n",
    "        diag_rew_dict = copy.deepcopy(test_env.debug_reward_dict)\n",
    "\n",
    "        test_env.unwrapped.debug_actions\n",
    "        \n",
    "        eval_vec[timestep, 0] = obs[0]\n",
    "        eval_vec[timestep, 1] = obs[9]\n",
    "        eval_vec[timestep, 2] = test_env.unwrapped.avg_vel\n",
    "        eval_vec[timestep, 3] = diag_rew_dict['v_tgt'] * reward_wt_param['v_tgt']\n",
    "        eval_vec[timestep, 4] = diag_rew_dict['alive_rew'] * reward_wt_param['alive_rew']\n",
    "        eval_vec[timestep, 5] = diag_rew_dict['footstep'] * reward_wt_param['footstep']\n",
    "        eval_vec[timestep, 6] = diag_rew_dict['effort'] * reward_wt_param['effort']\n",
    "        eval_vec[timestep, 7] = diag_rew_dict['action_penalty_zero'] * reward_wt_param['action_penalty_zero']\n",
    "        \n",
    "        # Collect data\n",
    "        if action_queue.shape[0] != 1:\n",
    "            action_queue = np.vstack((action_queue, action))\n",
    "        else:\n",
    "            action_queue = action\n",
    "            \n",
    "        ep_reward += rewards\n",
    "\n",
    "        if timestep == max_timestep-1:\n",
    "            print(f\"internal time step: {test_env.unwrapped.time_step}\")\n",
    "            print(f\"Terminated: {is_done}, Truncated : {failed}\")\n",
    "        \n",
    "        if is_done:\n",
    "            print(f\"Succed at {timestep}\")\n",
    "            break\n",
    "        if failed:\n",
    "            print(f\"Failed at {timestep}\")\n",
    "            break\n",
    "        \n",
    "    eval_dict = {'obs': obs_list, 'best_action_seq': action_queue, 'best_reward': ep_reward, \n",
    "                 'longest_action_seq': action_queue, 'longest_reward': ep_reward}\n",
    "\n",
    "    print(f\"Recorded best reward: {ep_reward}, ep_len: {action_queue.shape[0]}\")    \n",
    "    \n",
    "    # skvideo.io.vwrite(os.path.join(path, f\"{filename}_30Hz.mp4\"),\n",
    "    #                   np.asarray(frames),inputdict={\"-r\":\"100\"}, outputdict={\"-r\" : \"100\", \"-pix_fmt\": \"yuv420p\"})\n",
    "    # print('Best ep video rendered')\n",
    "\n",
    "    if action_queue.shape[0] != eval_vec.shape[0]:\n",
    "        eval_vec = eval_vec[0:action_queue.shape[0], :]\n",
    "    \n",
    "    # Average velocity plotting\n",
    "    step_idx = np.where(eval_vec[0:action_queue.shape[0],2] > 0)[0]\n",
    "    step_vec = eval_vec[0:action_queue.shape[0],2].copy()\n",
    "    \n",
    "    step_idx = np.hstack((np.array(0), step_idx))\n",
    "    \n",
    "    for idx in np.arange(len(step_idx)-1):\n",
    "        step_vec[step_idx[idx]:step_idx[idx+1]] = step_vec[step_idx[idx+1]]\n",
    "\n",
    "    # Assume the last step has the same average velocity as the last 2nd\n",
    "    step_vec[step_idx[-1]::] = step_vec[step_idx[-1]]\n",
    "    \n",
    "    fig_1 = plt.figure()\n",
    "    plt.plot(np.arange(action_queue.shape[0]), eval_vec[0:action_queue.shape[0],0], label='Target velocity')\n",
    "    plt.plot(np.arange(action_queue.shape[0]), eval_vec[0:action_queue.shape[0],1], label='Current velocity')\n",
    "    plt.plot(np.arange(action_queue.shape[0]), step_vec, label='Step velocity')\n",
    "    #plt.plot(np.arange(action_queue.shape[0]), eval_vec[0:action_queue.shape[0],2], label='Step velocity')\n",
    "    #plt.plot(np.arange(max_timestep), cvel_vec[:,0], label='COM velocity')\n",
    "    plt.legend()\n",
    "    fig_1.savefig(os.path.join(path, f\"{filename}_velocities.png\"))\n",
    "\n",
    "    # fig_2 = plt.figure()\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 3], label='Tgt velocity reward')\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 4], label='Footstep reward')\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 5], label='Effort reward')\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 6], label='Action Penalty reward')\n",
    "    # plt.legend()\n",
    "    # plt.title(f\"Total reward: {ep_reward}\")\n",
    "    # fig_2.savefig(os.path.join(path, f\"{filename}_rewards.png\"))\n",
    "\n",
    "    #print(eval_vec[:, 2:7])\n",
    "    \n",
    "    sumedRew = np.sum(eval_vec[:, 3:8], axis=0).tolist()\n",
    "    labels = ['Alive', 'Tgt_vel', 'Footstep', 'Effort', 'Action_Penalty']\n",
    "\n",
    "    fig_2, ax = plt.subplots(figsize =(16, 9))\n",
    "    ax.barh(labels, sumedRew)\n",
    "    \n",
    "    # Remove axes splines\n",
    "    for s in ['top', 'bottom', 'left', 'right']:\n",
    "        ax.spines[s].set_visible(False)\n",
    "        \n",
    "    # Remove x, y Ticks\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    \n",
    "    # Add padding between axes and labels\n",
    "    ax.xaxis.set_tick_params(pad = 5)\n",
    "    ax.yaxis.set_tick_params(pad = 10)\n",
    "    \n",
    "    # Add annotation to bars\n",
    "    for i in ax.patches:\n",
    "        plt.text(i.get_width()+0.2, i.get_y()+0.5, \n",
    "                 str(round((i.get_width()), 2)),\n",
    "                 fontsize = 10, fontweight ='bold',\n",
    "                 color ='grey')\n",
    "    ax.set_title(f\"Total reward: {ep_reward}\", loc ='left')\n",
    "    \n",
    "    # fig_2 = plt.figure()\n",
    "    # plt.subplot2grid((5, 4), (0, 0), rowspan=5, colspan=2)\n",
    "    # plt.barh(labels, sumedRew)\n",
    "    \n",
    "    # plt.subplot2grid((5, 4), (0, 3))\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 2], label='Alive reward')\n",
    "    # plt.ylabel('Alive reward')\n",
    "    # plt.subplot2grid((5, 4), (1, 3))\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 3], label='Tgt velocity reward')\n",
    "    # plt.ylabel('Tgt reward')\n",
    "    # plt.subplot2grid((5, 4), (2, 3))\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 4], label='Footstep reward')\n",
    "    # plt.ylabel('Footstep reward')\n",
    "    # plt.subplot2grid((5, 4), (3, 3))\n",
    "    # plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 5], label='Effort')\n",
    "    # plt.ylabel('Effort reward')\n",
    "    # #plt.subplot2grid((5, 4), (4, 3))\n",
    "    # #plt.plot(np.arange(action_queue.shape[0]), eval_vec[:, 6], label='Action Penalty')\n",
    "    # #plt.ylabel('Action penalty')\n",
    "    \n",
    "    #plt.suptitle(f\"Total reward: {ep_reward}\")\n",
    "    fig_2.savefig(os.path.join(path, f\"{filename}_rewards.png\"))\n",
    "    \n",
    "    fig_3 = plt.figure()\n",
    "    plt.plot(action_queue)\n",
    "    fig_3.savefig(os.path.join(path, f\"{filename}_Actions.png\"))\n",
    "\n",
    "    \n",
    "    plt.close(fig_1)\n",
    "    plt.close(fig_2)\n",
    "    plt.close(fig_3)\n",
    "    \n",
    "    return eval_dict, eval_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d018f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32,\n",
    "                              np.float64)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, (np.ndarray,)):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "def save_data_plots(eval_dict, path, filename):\n",
    "    fig_1 = plt.figure()\n",
    "    plt.subplot(2,1,1)\n",
    "    plt.hist(eval_dict['all_ep_len'], bins=np.arange(0,2001,200))\n",
    "    ep_success = np.where(np.array(eval_dict['all_ep_len']) == 2000)[0].shape[0]\n",
    "    plt.title(f\"MyoReflexRL - Ep length distributions (Success: {ep_success / len(eval_dict['all_ep_len'])})\")\n",
    "    \n",
    "    plt.subplot(2,1,2)\n",
    "    plt.hist(eval_dict['all_rewards'], bins=np.arange(0,501,50))\n",
    "    \n",
    "    fig_1.savefig(os.path.join(path, f\"{filename}_eval_stats.png\"))\n",
    "\n",
    "    np.save( os.path.join(main_path, f\"{directory_list[idx]}_{suffix}_eval\"), eval_dict)\n",
    "    # dumped = json.dumps(eval_dict, cls=NumpyEncoder)\n",
    "    # with open(os.path.join(path, f\"{filename}_eval.json\"), 'a') as f:\n",
    "    #     f.write(dumped + '\\n')\n",
    "    # Use numpy save instead to save dict\n",
    "\n",
    "def extract_reward_terms(test_env, best_replay, max_timestep, vel_sequence, path, filename):\n",
    "    # Additional section to track the current and target velocitiy of the best\n",
    "    test_env.unwrapped.eval_x_vel = vel_sequence[0]\n",
    "    #test_env.unwrapped.target_vel_type = 'constant'\n",
    "    \n",
    "    test_env.reset()\n",
    "    \n",
    "    # 0 - Tgt velocity (obs)\n",
    "    # 1 - Current velocity\n",
    "    # 2 - tgt reward\n",
    "    # 3 - avg step velocity\n",
    "    # 4 - foot step (boolean) - As we only get tgt reward when there is a step\n",
    "    eval_vec = np.zeros((max_timestep, 6))\n",
    "    cvel_vec = np.zeros((max_timestep, 1))\n",
    "    #print(f\"In func:{test_env.target_x_vel}\")\n",
    "    \n",
    "    for timestep in range(max_timestep):\n",
    "        test_env.unwrapped.eval_x_vel = vel_sequence[timestep]\n",
    "        \n",
    "        obs, _, is_done, failed, _ = test_env.step(best_replay[timestep,:])\n",
    "        # Getting rewarddict of current step\n",
    "        curr_reward_dict = copy.deepcopy(test_env.get_reward_dict_old(is_done, best_replay[timestep,:]))\n",
    "        \n",
    "        eval_vec[timestep, 0] = obs[0]\n",
    "        eval_vec[timestep, 1] = obs[9]\n",
    "        eval_vec[timestep, 2] = curr_reward_dict['alive']\n",
    "        eval_vec[timestep, 3] = curr_reward_dict['v_tgt']\n",
    "        eval_vec[timestep, 4] = curr_reward_dict['footstep']\n",
    "        eval_vec[timestep, 5] = curr_reward_dict['effort']\n",
    "\n",
    "        # mass = np.expand_dims(eval_env.unwrapped.MyoEnv.sim.model.body_mass, -1) \n",
    "        # cvel = eval_env.unwrapped.MyoEnv.sim.data.cvel\n",
    "        # cvel_vec[timestep,0] = (np.sum(mass * cvel, 0) / np.sum(mass))[3]\n",
    "\n",
    "    fig_1 = plt.figure()\n",
    "    plt.plot(np.arange(max_timestep), eval_vec[:,0], label='Target velocity')\n",
    "    plt.plot(np.arange(max_timestep), eval_vec[:,1], label='Current velocity')\n",
    "    #plt.plot(np.arange(max_timestep), cvel_vec[:,0], label='COM velocity')\n",
    "    plt.legend()\n",
    "    fig_1.savefig(os.path.join(path, f\"{filename}_velocities.png\"))\n",
    "\n",
    "    fig_2 = plt.figure()\n",
    "    plt.plot(np.arange(max_timestep), eval_vec[timestep, 3], label='Tgt velocity reward')\n",
    "    plt.plot(np.arange(max_timestep), eval_vec[timestep, 4], label='Footstep reward')\n",
    "    plt.plot(np.arange(max_timestep), eval_vec[timestep, 5], label='Effort reward')\n",
    "    plt.legend()\n",
    "    fig_2.savefig(os.path.join(path, f\"{filename}_rewards.png\"))\n",
    "\n",
    "    plt.close(fig_1)\n",
    "    plt.close(fig_2)\n",
    "    \n",
    "    return eval_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040b8c9-f5a1-4012-b3da-6eeacdfde9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skvideo.io\n",
    "\n",
    "def save_video_replay(test_env, eval_dict, path, filename):\n",
    "    # Generating the best result\n",
    "    # best_action and best_reward\n",
    "\n",
    "    best_reward = eval_dict['best_reward']\n",
    "    best_ep_len = len(eval_dict['best_action_seq'])\n",
    "    longest_reward = eval_dict['longest_reward']\n",
    "    longest_ep_len = len(eval_dict['longest_action_seq'])\n",
    "    \n",
    "    print(f\"Recorded best reward: {best_reward}, ep_len: {best_ep_len}\")    \n",
    "    print(f\"Recorded longest reward: {longest_reward}, ep_len: {longest_ep_len}\")\n",
    "\n",
    "    # Recording 2 videos, instead of 1\n",
    "    out_action = eval_dict['best_action_seq']\n",
    "    \n",
    "    frames = []\n",
    "    data_store = []\n",
    "    #out_rewards = np.zeros((len(out_action), ))\n",
    "    \n",
    "    obs, _ = test_env.reset()\n",
    "    \n",
    "    for time_step in range(len(out_action)):\n",
    "        frame = test_env.MyoEnv.sim.renderer.render_offscreen(camera_id=1)\n",
    "        frames.append(frame)\n",
    "    \n",
    "        obs, rewards, is_done, failed, info = test_env.step(out_action[time_step,:])\n",
    "            \n",
    "        if is_done:\n",
    "            #print(f\"Succed at {time_step}\")\n",
    "            break\n",
    "    skvideo.io.vwrite(os.path.join(path, f\"{filename}_best_reward_video.mp4\"),\n",
    "                      np.asarray(frames),inputdict={\"-r\":\"33\"}, outputdict={\"-r\" : \"100\", \"-pix_fmt\": \"yuv420p\"})\n",
    "    print('Best reward video rendered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed140026-6482-426a-9f49-6e751b6761e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_constant_vel(test_env, test_policy, path, filename, reward_wt_param, velocities=np.array([0.8, 1.2, 1.6, 1.8]), max_timestep=2000):\n",
    "    # 0.8, 1.2, 1.6, 1.8\n",
    "    for vel_idx in range(velocities.shape[0]):\n",
    "        print(f\"Evaluating constant velocity: {velocities[vel_idx]}\")\n",
    "        vel_seq = np.ones(max_timestep)*velocities[vel_idx]\n",
    "        convert = str(velocities[vel_idx]).replace('.','_')\n",
    "        \n",
    "        newfilename = f\"{filename}_constant_{convert}\"\n",
    "\n",
    "        eval_dict, eval_vec = evaluate_30Hz_policy(test_env, test_policy, max_timestep, vel_seq, path, newfilename, reward_wt_param)\n",
    "    #return eval_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d871083-1f7b-446e-bea0-c9ca61256dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_change_vel(test_env, test_policy, path, filename, reward_wt_param, velocities=np.array([0.8, 1.2, 1.6, 1.8]), max_timestep=2000):\n",
    "\n",
    "    velocities = np.array([[1.2, 0.8], [1.2, 1.6], [0.8, 1.6], [1.6, 0.8], [0.8, 1.8]])\n",
    "    #seq = np.zeros((5, 2000))\n",
    "    \n",
    "    for vel_idx in range(velocities.shape[0]):\n",
    "        print(f\"Evaluating changing velocity: {velocities[vel_idx, :]}\")\n",
    "        vel_seq = np.ones(max_timestep)*velocities[vel_idx, 0]\n",
    "        vel_seq[1000:2000] = velocities[vel_idx, 1]\n",
    "        #seq[vel_idx, :] = vel_seq.copy() \n",
    "        convert = str(velocities[vel_idx,0]).replace('.','_')\n",
    "        convert_2 = str(velocities[vel_idx,1]).replace('.','_')\n",
    "        \n",
    "        newfilename = f\"{filename}_change_{convert}_to_{convert_2}\"\n",
    "        \n",
    "        eval_dict, eval_vec = evaluate_30Hz_policy(test_env, test_policy, max_timestep, vel_seq, path, newfilename, reward_wt_param)\n",
    "        \n",
    "        #print(f\"Vel seq {vel_seq}\")\n",
    "        #print(f\"path: {path}, filename: {newfilename}\")\n",
    "        #eval_dict = evaluate_trained_policy(test_env, test_policy, max_timestep=max_timestep, n_eval_ep=1, vel_sequence=vel_seq)\n",
    "        #extract_reward_terms(test_env, np.array(eval_dict['best_action_seq']), max_timestep=np.array(eval_dict['best_action_seq']).shape[0], vel_sequence=vel_seq, \n",
    "        #                     path=path, filename=newfilename)\n",
    "        #print(\"Rendering video...\")\n",
    "        #save_video_replay(test_env, eval_dict, path=path, filename=newfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bf23e-d291-478a-8e08-fc0272d1db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample\n",
    "def evaluate_sinusoidal_vel(test_env, test_policy, path, filename, reward_wt_param, velocities=np.array([0.8, 1.2, 1.6, 1.8]), max_timestep=2000):\n",
    "\n",
    "    velocities = np.array([0.8, 1.4])\n",
    "    periods = np.array([2000, 1000, 500, 300])\n",
    "    seq = np.zeros((4, 2000))\n",
    "    \n",
    "    for vel_idx in range(periods.shape[0]):\n",
    "        print(f\"Evaluating sinusoidal velocity: {velocities} on period {periods[vel_idx]}\")\n",
    "        \n",
    "        vel_seq = np.zeros(2000,)\n",
    "        for time in range(2000):\n",
    "            vel_seq[time] = get_sinusoidal_vel(0.8, 1.4, periods[vel_idx], time)[0]\n",
    "        \n",
    "        convert = str(velocities[0]).replace('.','_')\n",
    "        convert_2 = str(velocities[1]).replace('.','_')\n",
    "        \n",
    "        newfilename = f\"{filename}_sine_{convert}_to_{convert_2}_period_{periods[vel_idx]}\"\n",
    "        \n",
    "        eval_dict, eval_vec = evaluate_30Hz_policy(test_env, test_policy, max_timestep, vel_seq, path, newfilename, reward_wt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79894c1b-f6db-4ba8-9863-3e9eae0f94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sinusoidal_vel(sine_min, sine_max, sine_period, current_time):\n",
    "    \"\"\"\n",
    "    Compute the value of a sine wave at a specific time.\n",
    "    Current time: Given in milliseconds\n",
    "    \"\"\"\n",
    "    #phase_shift = 0\n",
    "    \n",
    "    amplitude = (sine_max - sine_min) / 2\n",
    "    offset = (sine_min + sine_max) / 2\n",
    "\n",
    "    frequency = 1 / sine_period\n",
    "    value = amplitude * np.sin(2 * np.pi * frequency * current_time) + offset\n",
    "\n",
    "    return np.array([value, 0]) # Currently only for 2D walking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6174048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripting to automatically generate all the reports and videos in a loop. Eventually make it into a python file\n",
    "import os\n",
    "from glob import glob\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# main_path = os.path.join('params', '11mus_RL', '2024_02_29', 'Asym_Simplified_256_128_RealTime') # , '2024_01_31_to_02_05', 'TgtPosVel'\n",
    "main_path = os.path.join('reflex_RL', 'reflex_param', 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best') # , '2024_01_31_to_02_05', 'TgtPosVel'\n",
    "param_filename = 'myorfl_Kine_2D_1_25_2024Nov25_1718_22mus_DelayKine_Best'\n",
    "params = np.loadtxt(os.path.join(main_path, f\"{param_filename}.txt\"))\n",
    "\n",
    "obs_param = None #['spinal_phase', 'mus_f', 'mus_l', 'mus_v']\n",
    "reward_dict = {'alive_rew': 1, 'footstep': 1, 'action_penalty_zero': 0.1, 'effort': 0, 'v_tgt': 5}\n",
    "#reward_dict = {'alive_rew': 1, 'footstep': 1, 'v_tgt': 1}\n",
    "reward_func = 2\n",
    "\n",
    "# Modify to change evaluation environment\n",
    "# Fixed constant velocity\n",
    "tgt_vel = np.array([1.25,0])\n",
    "sine_vel_args=None\n",
    "\n",
    "# Randomized constant velocity\n",
    "# tgt_vel = np.array([-1,-1])\n",
    "# tgt_vel_mode='constant'\n",
    "# sine_vel_args=None\n",
    "\n",
    "# Fixed Sinusoidal velocity\n",
    "# tgt_vel = np.array([-1,-1])\n",
    "# tgt_vel_mode='sine' #'sine'\n",
    "# sine_vel_args=dict(sine_min=0.8, sine_max=1.8, sine_period=4000)\n",
    "\n",
    "# Randomized Sinusoidal velocity\n",
    "# tgt_vel = np.array([-1,-1])\n",
    "# tgt_vel_mode='sine' #'sine'\n",
    "# sine_vel_args=None\n",
    "\n",
    "eval_env = gymnasium.make('MyoReflex_RL-v0', init_pose='walk', dt=0.01, mode='2D', tgt_field_ver=0, reflex_params=params, episode_limit=2000, \n",
    "                          obs_param=obs_param, rew_type=reward_func, reward_wt=reward_dict, stim_mode='reflex', \n",
    "                          target_vel=tgt_vel, tgt_vel_mode='eval', sine_vel_args=sine_vel_args, delta_mode='realtime', delta_control_mode='sym')\n",
    "eval_env.reset()\n",
    "\n",
    "# test_model = PPO.load(\"params/11mus_RL/2023_12_27/Obs_dist/2D_PPO_rewOld_Obs_dist_128_128/PPO_2023Dec27_2006_2D_TrainedModel.zip\", env=eval_env)\n",
    "test_model = PPO.load(\"reflex_RL/PPO_outputV3/PPO_2023Dec27_2006_2D_TrainedModel.zip\", env=eval_env)\n",
    "\n",
    "gen_path = os.listdir(main_path)\n",
    "\n",
    "directory_list = [gen_path[i] for i in range(len(gen_path)) if os.path.isdir(os.path.join(main_path, gen_path[i]))]\n",
    "\n",
    "print(f\"Directory: {directory_list}\")\n",
    "\n",
    "for idx in range(len(directory_list)): # len(directory_list)\n",
    "    print(glob( os.path.join(main_path, directory_list[idx], '*.zip') ))\n",
    "    #print(directory_list[idx])\n",
    "\n",
    "    test_model = PPO.load(glob( os.path.join(main_path, directory_list[idx], '*.zip') )[0])\n",
    "    # suffix = str(0.8).replace('.','_')\n",
    "\n",
    "    evaluate_constant_vel(eval_env, test_model, main_path, f\"{directory_list[idx]}\", reward_dict)\n",
    "    #evaluate_change_vel(eval_env, test_model, main_path, f\"{directory_list[idx]}\", reward_dict)\n",
    "    #evaluate_sinusoidal_vel(eval_env, test_model, main_path, f\"{directory_list[idx]}\", reward_dict)\n",
    "    \n",
    "    print('Done')\n",
    "    #eval_dict = evaluate_trained_policy(eval_env, test_model, vel_seq.shape[0], 1, vel_sequence=vel_seq, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    #save_data_plots(eval_dict, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    #extract_reward_terms(eval_env, np.array(eval_dict['best_action_seq']), np.array(eval_dict['best_action_seq']).shape[0], vel_sequence=vel_seq, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    #save_video_replay(eval_env, eval_dict, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    \n",
    "    # out_dict = evaluate_30Hz_policy(test_env=eval_env, test_policy=test_model, max_timestep=vel_seq.shape[0], \n",
    "    #                                  vel_sequence=vel_seq, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    # np.save( os.path.join(main_path, f\"{directory_list[idx]}_{suffix}_out_vel\"), out_dict)\n",
    "    # print('Done')\n",
    "    \n",
    "    # for tgt_vel in target_vel:\n",
    "    \n",
    "    #     eval_env = gymnasium.make('MyoReflex_RL-v0', init_pose='walk', dt=0.01, mode='2D', tgt_field_ver=0, reflex_params=params, episode_limit=2000, \n",
    "    #                               obs_param=obs_param, rew_type=reward_func, reward_wt=reward_dict, \n",
    "    #                               target_vel=np.array([tgt_vel, 0]), tgt_vel_mode=tgt_vel_mode, sine_vel_args=sine_vel_args)\n",
    "    #     eval_env.reset()\n",
    "    \n",
    "    #     suffix = str(tgt_vel).replace('.','_')\n",
    "    \n",
    "    #     eval_dict = evaluate_trained_policy(eval_env, test_model, 2000, 10, target_x_vel=tgt_vel)\n",
    "    \n",
    "    #     save_data_plots(eval_dict, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    #     extract_reward_terms(eval_env, np.array(eval_dict['best_action_seq']), 2000, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    #     save_video_replay(eval_env, eval_dict, path=main_path, filename=f\"{directory_list[idx]}_{suffix}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CR_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
